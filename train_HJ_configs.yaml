
# Dubins car environment parameters
speed: 1.
turnRate: 1.25
x_min: -3
x_max: 3
y_min: -3
y_max: 3 
buffer: 0.1
dt: 0.05
obs_x: 0
obs_y: 0
obs_r: 0.5

# offline data parameters
data_length: 100
num_trajs: 4000 # 2000 in paper
num_train_trajs: 3800 #1900 in paper
size: [128, 128]
dataset_path: 'wm_demos128.pkl' # '128' should be the same as the size above

# offline dreamer parameters, these are from https://github.com/NM512/dreamerv3-torch
logdir: '/storage1/sibai/Active/ihab/research_new/dino_wm' 
rssm_ckpt_path: logs/dreamer_dubins/rssm_ckpt.pt # change this to whatever the path to your rssm checkpoint is
traindir: null
evaldir: null
offline_traindir: ''
offline_evaldir: ''
seed: 0
deterministic_run: False
steps: 1e6
parallel: False
eval_every: 5e2
eval_episode_num: 10
log_every: 5e2
reset_every: 0
device: 'cuda:0'
compile: True
precision: 32
debug: False
video_pred_log: True

rssm_train_steps: 10000 # 100000 in paper

# Environment
task: 'dubins-wm'
envs: 1
action_repeat: 1
time_limit: 100
grayscale: False
prefill: 5000
reward_EMA: True

# Model
dyn_hidden: 512
dyn_deter: 512
dyn_stoch: 32
dyn_discrete: 0 # 0 for continuous latent
dyn_rec_depth: 1
dyn_mean_act: 'none'
dyn_std_act: 'sigmoid2'
dyn_min_std: 0.1
grad_heads: ['decoder', "margin", "cont"]
units: 512
act: 'SiLU'
norm: True
encoder:
  {mlp_keys: 'obs_state', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
decoder:
  {mlp_keys: 'obs_state', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, cnn_sigmoid: False, image_dist: mse, vector_dist: symlog_mse, outscale: 1.0}
actor:
  {layers: 2, dist: 'normal', entropy: 3e-4, unimix_ratio: 0.01, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
critic:
  {layers: 2, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
#reward_head:
#  {layers: 2, dist: 'symlog_disc', loss_scale: 1.0, outscale: 0.0}
cont_head:
  {layers: 2, loss_scale: 1.0, outscale: 1.0}
# new
margin_head:
  {layers: 2, loss_scale: 10.0}
gamma_lx: 0.75 
dyn_scale: 0.5
rep_scale: 0.1
kl_free: 1.0
weight_decay: 0.0
unimix_ratio: 0.01
initial: 'learned'

# Training
batch_size: 16
batch_length: 64
train_ratio: 512
pretrain: 100
model_lr: 1e-4
obs_lr: 1e-3
lx_lr: 1e-4
opt_eps: 1e-8
grad_clip: 1000
dataset_size: 1000000
opt: 'adam'

# Behavior.
discount: 0.997
discount_lambda: 0.95
imag_horizon: 25
imag_gradient: 'dynamics'
imag_gradient_mix: 0.0
eval_state_mean: False

# Exploration
expl_behavior: 'greedy'
expl_until: 0
expl_extr_scale: 0.0
expl_intr_scale: 1.0
disag_target: 'stoch'
disag_log: True
disag_models: 10
disag_offset: 1
disag_layers: 4
disag_units: 400
disag_action_cond: False


# LCRL
reward-threshold: null 
#seed: 0 
buffer-size: 40000  #40000 # changed this from 40000 to 400000
actor-lr: 1e-4
critic-lr: 1e-3
gamma-pyhj: 0.9999 # type=float, default=0.95)
tau: 0.005 # type=float, default=0.005)
exploration-noise: 0.1 # type=float, default=0.1)
epoch: 1 # type=int, default=10)
total-episodes: 120 # type=int, default=160)
step-per-epoch: 2000 # type=int, default=40000)
step-per-collect: 8 # type=int, default=8)
update-per-step: 0.125 # type=float, default=0.125)
batch_size-pyhj: 512 # type=int, default=512)
control-net: [512, 512, 512] # type=int, nargs='*', default=None) # for control policy
critic-net: [512, 512, 512]  # type=int, nargs='*', default=None) # for critic net
# control-net: [256, 256, 256] # type=int, nargs='*', default=None) # for control policy
# critic-net: [256, 256, 256]  # type=int, nargs='*', default=None) # for critic net
training-num: 4 # type=int, default=8)
test-num: 2 # type=int, default=100)
render: 0. # type=float, default=0.)
rew-norm: False # action="store_true", default=False)
n-step: 1 # type=int, default=1)
continue-training-logdir: null # type=str, default=None)
continue-training-epoch: null # type=int, default=None)
actor-gradient-steps: 1 # type=int, default=1)
is-game-baseline: False # type=bool, default=False) # it will be set automatically
target-update-freq: 400 # type=int, default=400)
auto-alpha: 1
alpha-lr: 3e-4
alpha: 0.2 
weight-decay-pyhj: 0.001

actor-activation: 'ReLU' #type=str, default='ReLU')
critic-activation: 'ReLU' # type=str, default='ReLU')
kwargs: {} # type=str, default='{}')
warm-start-path: null # type=str, default=None) # e.g., log/ra_droneracing_Game-v6/epoch_id_10/policy.pth

nx: 100
ny: 100

# params for latent h
freeze_wm: True
single_layer_classifier: False
with_proprio: True
expert_warmup: True
finetune_backbone: False
expert_ckpt_path: "/storage1/sibai/Active/ihab/research_new/ManiSkill/examples/baselines/ppo/runs/UnitreeG1PlaceAppleInBowl-v1__ppo__8__1750699220/final_ckpt.pt"