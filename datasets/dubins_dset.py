# import torch
# import decord
# import numpy as np
# from pathlib import Path
# from einops import rearrange
# from decord import VideoReader
# from typing import Callable, Optional
# from .traj_dset import TrajDataset, get_train_val_sliced
# from typing import Optional, Callable, Any
# decord.bridge.set_bridge("torch")

# class PointMazeDataset(TrajDataset):
#     def __init__(
#         self,
#         data_path: str = "data/dubins_dataset",
#         n_rollout: Optional[int] = None,
#         transform: Optional[Callable] = None,
#         normalize_action: bool = False,
#         action_scale=1.0,
#     ):
#         self.data_path = Path(data_path)
#         self.transform = transform
#         self.normalize_action = normalize_action
#         states = torch.load(self.data_path / "states.pth").float()
#         self.states = states
#         self.actions = torch.load(self.data_path / "actions.pth").float()
#         self.actions = self.actions / action_scale  # scaled back up in env
#         self.seq_lengths = torch.load(self.data_path /'seq_lengths.pth')

#         self.n_rollout = n_rollout
#         if self.n_rollout:
#             n = self.n_rollout
#         else:
#             n = len(self.states)

#         self.states = self.states[:n]
#         self.actions = self.actions[:n]
#         self.seq_lengths = self.seq_lengths[:n]
#         self.proprios = self.states.clone()
#         print(f"Loaded {n} rollouts")

#         self.action_dim = self.actions.shape[-1]
#         self.state_dim = self.states.shape[-1]
#         self.proprio_dim = self.proprios.shape[-1]

#         if normalize_action:
#             self.action_mean, self.action_std = self.get_data_mean_std(self.actions, self.seq_lengths)
#             self.state_mean, self.state_std = self.get_data_mean_std(self.states, self.seq_lengths)
#             self.proprio_mean, self.proprio_std = self.get_data_mean_std(self.proprios, self.seq_lengths)
#         else:
#             self.action_mean = torch.zeros(self.action_dim)
#             self.action_std = torch.ones(self.action_dim)
#             self.state_mean = torch.zeros(self.state_dim)
#             self.state_std = torch.ones(self.state_dim)
#             self.proprio_mean = torch.zeros(self.proprio_dim)
#             self.proprio_std = torch.ones(self.proprio_dim)

#         self.actions = (self.actions - self.action_mean) / self.action_std
#         self.proprios = (self.proprios - self.proprio_mean) / self.proprio_std
    
#     def get_data_mean_std(self, data, traj_lengths):
#         all_data = []
#         for traj in range(len(traj_lengths)):
#             traj_len = traj_lengths[traj]
#             traj_data = data[traj, :traj_len]
#             all_data.append(traj_data)
#         all_data = torch.vstack(all_data)
#         data_mean = torch.mean(all_data, dim=0)
#         data_std = torch.std(all_data, dim=0)
#         return data_mean, data_std

#     def get_seq_length(self, idx):
#         return self.seq_lengths[idx]

#     def get_all_actions(self):
#         result = []
#         for i in range(len(self.seq_lengths)):
#             T = self.seq_lengths[i]
#             result.append(self.actions[i, :T, :])
#         return torch.cat(result, dim=0)

#     def get_frames(self, idx, frames):
#         obs_dir = self.data_path / "obses"
#         image = torch.load(obs_dir / f"episode_{idx:03d}.pth")
#         proprio = self.proprios[idx, frames]
#         act = self.actions[idx, frames]
#         state = self.states[idx, frames]

#         image = image[frames]  # THWC
#         image = image / 255.0
#         image = rearrange(image, "T H W C -> T C H W")
#         if self.transform:
#             image = self.transform(image)
#         obs = {
#             "visual": image,
#             "proprio": proprio
#         }
#         return obs, act, state, {} # env_info

#     def __getitem__(self, idx):
#         return self.get_frames(idx, range(self.get_seq_length(idx)))

#     def __len__(self):
#         return len(self.seq_lengths)

#     def preprocess_imgs(self, imgs):
#         if isinstance(imgs, np.ndarray):
#             raise NotImplementedError
#         elif isinstance(imgs, torch.Tensor):
#             return rearrange(imgs, "b h w c -> b c h w") / 255.0
        
# def load_dubins_slice_train_val(
#     transform,
#     n_rollout=50,
#     data_path='data/dubins_dataset',
#     normalize_action=False,
#     split_ratio=0.8,
#     num_hist=0,
#     num_pred=0,
#     frameskip=0,
# ):
#     dset = PointMazeDataset(
#         n_rollout=n_rollout,
#         transform=transform,
#         data_path=data_path,
#         normalize_action=normalize_action,
#     )
#     dset_train, dset_val, train_slices, val_slices = get_train_val_sliced(
#         traj_dataset=dset, 
#         train_fraction=split_ratio, 
#         num_frames=num_hist + num_pred, 
#         frameskip=frameskip
#     )

#     datasets = {}
#     datasets['train'] = train_slices
#     datasets['valid'] = val_slices
#     traj_dset = {}
#     traj_dset['train'] = dset_train
#     traj_dset['valid'] = dset_val
#     return datasets, traj_dset

import torch
import decord
import numpy as np
from pathlib import Path
from einops import rearrange
from decord import VideoReader
from typing import Callable, Optional
from .traj_dset import TrajDataset, get_train_val_sliced
decord.bridge.set_bridge("torch")

class PointMazeDataset(TrajDataset):
    def __init__(
        self,
        data_path: str = "data/dubins_dataset",
        n_rollout: Optional[int] = None,
        transform: Optional[Callable] = None,
        normalize_action: bool = False,
        action_scale=1.0,
    ):
        self.data_path = Path(data_path)
        self.transform = transform
        self.normalize_action = normalize_action

        # Load dataset from disk (list of tensors)
        self.states = torch.load(self.data_path / "states.pth")  # list of [T_i, 9]
        self.actions = torch.load(self.data_path / "actions.pth")  # list of [T_i, 1]
        self.seq_lengths = torch.load(self.data_path / 'seq_lengths.pth')  # list of ints

        # Truncate rollouts if needed
        if n_rollout:
            self.states = self.states[:n_rollout]
            self.actions = self.actions[:n_rollout]
            self.seq_lengths = self.seq_lengths[:n_rollout]

        # Cast to float and scale
        self.states = [s.float() for s in self.states]
        self.actions = [a.float() / action_scale for a in self.actions]
        self.proprios = [s[:, :3].clone() for s in self.states]

        print(f"Loaded {len(self.states)} rollouts")

        self.n_rollout = len(self.states)
        self.action_dim = self.actions[0].shape[-1]
        self.state_dim = self.states[0].shape[-1]
        self.proprio_dim = self.proprios[0].shape[-1]

        if normalize_action:
            self.action_mean, self.action_std = self.get_data_mean_std(self.actions)
            self.state_mean, self.state_std = self.get_data_mean_std(self.states)
            self.proprio_mean, self.proprio_std = self.get_data_mean_std(self.proprios)
        else:
            self.action_mean = torch.zeros(self.action_dim)
            self.action_std = torch.ones(self.action_dim)
            self.state_mean = torch.zeros(self.state_dim)
            self.state_std = torch.ones(self.state_dim)
            self.proprio_mean = torch.zeros(self.proprio_dim)
            self.proprio_std = torch.ones(self.proprio_dim)

        self.actions = [(a - self.action_mean) / self.action_std for a in self.actions]
        self.proprios = [(p - self.proprio_mean) / self.proprio_std for p in self.proprios]

    def get_data_mean_std(self, data_list):
        all_data = torch.cat(data_list, dim=0)
        return torch.mean(all_data, dim=0), torch.std(all_data, dim=0)

    def get_seq_length(self, idx):
        return self.seq_lengths[idx]

    def get_all_actions(self):
        return torch.cat([a[:T] for a, T in zip(self.actions, self.seq_lengths)], dim=0)

    def get_frames(self, idx, frames):
        obs_dir = self.data_path / "obses"
        image = torch.load(obs_dir / f"episode_{idx:03d}.pth")  # [T, 224, 224, 3]

        # Index sequence
        image = image[frames]  # THWC
        image = image / 255.0
        image = rearrange(image, "T H W C -> T C H W")

        if self.transform:
            image = self.transform(image)

        proprio = self.proprios[idx][frames]
        act = self.actions[idx][frames]
        state = self.states[idx][frames]

        obs = {
            "visual": image,
            "proprio": proprio
        }
        return obs, act, state, {}  # env_info placeholder

    def __getitem__(self, idx):
        return self.get_frames(idx, range(self.get_seq_length(idx)))

    def __len__(self):
        return len(self.seq_lengths)

    def preprocess_imgs(self, imgs):
        if isinstance(imgs, np.ndarray):
            raise NotImplementedError
        elif isinstance(imgs, torch.Tensor):
            return rearrange(imgs, "b h w c -> b c h w") / 255.0

def load_dubins_slice_train_val(
    transform,
    n_rollout=50,
    data_path='data/dubins_dataset',
    normalize_action=False,
    split_ratio=0.8,
    num_hist=0,
    num_pred=0,
    frameskip=0,
):
    dset = PointMazeDataset(
        n_rollout=n_rollout,
        transform=transform,
        data_path=data_path,
        normalize_action=normalize_action,
    )
    dset_train, dset_val, train_slices, val_slices = get_train_val_sliced(
        traj_dataset=dset, 
        train_fraction=split_ratio, 
        num_frames=num_hist + num_pred, 
        frameskip=frameskip
    )

    datasets = {
        'train': train_slices,
        'valid': val_slices
    }
    traj_dset = {
        'train': dset_train,
        'valid': dset_val
    }
    return datasets, traj_dset
